{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "my_tar = tarfile.open('aclImdb_v1.tar.gz')\n",
    "my_tar.extractall('./') # specify which folder to extract to\n",
    "my_tar.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filepath = filepath + \"/pos\"\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def createdata(filename, tag):\n",
    "    ratings = []\n",
    "    reviews= []\n",
    "    index_ = []\n",
    "    for file in os.walk(filename):\n",
    "        dir_ = file[0]\n",
    "        for i in file[2]:\n",
    "            rating = i.split(\".\")[0]\n",
    "            index = rating.split(\"_\")[0]\n",
    "            index_.append(index)\n",
    "            rating = rating.split(\"_\")[1]\n",
    "            ratings.append(rating)\n",
    "            \n",
    "            with open(dir_ + \"\\\\\" + i, encoding=\"utf8\") as f:\n",
    "                review = f.read()\n",
    "                reviews.append(review)\n",
    "\n",
    "    mydf = pd.DataFrame(list(zip(reviews, ratings)), columns = ['Reviews', 'Ratings'])\n",
    "    return mydf\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neg = createdata(r\"C:\\Users\\Asmeeta\\Documents\\AdaptReady\\aclImdb\\train\\neg\", \"neg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos = createdata(r\"C:\\Users\\Asmeeta\\Documents\\AdaptReady\\aclImdb\\train\\pos\", \"pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neg_test = createdata(r\"C:\\Users\\Asmeeta\\Documents\\AdaptReady\\aclImdb\\test\\neg\", \"neg\")\n",
    "df_pos_test = createdata(r\"C:\\Users\\Asmeeta\\Documents\\AdaptReady\\aclImdb\\test\\pos\", \"pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a smaller dataset\n",
    "def less_data(df_pos, df_neg):\n",
    "    top_df_pos = df_pos.iloc[0:1250,:]\n",
    "    top_df_neg = df_neg.iloc[0:1250,:]\n",
    "    frames = [top_df_pos, top_df_neg]\n",
    "    df_ = pd.concat(frames)\n",
    "    len(df_)\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = less_data(df_pos_test,df_neg_test)\n",
    "len(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = less_data(df_pos,df_neg)\n",
    "len(df_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"less_train_data.csv\")\n",
    "df_test.to_csv(\"less_test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelmaker(x):\n",
    "    x = float(x)\n",
    "    if x >= 7:\n",
    "        return 1\n",
    "    elif x <= 4:\n",
    "        return 0\n",
    "    else :\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Label\"] = df_train[\"Ratings\"].apply(labelmaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1250\n",
       "0    1250\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"Label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1250\n",
       "0    1250\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[\"Label\"] = df_train[\"Ratings\"].apply(labelmaker)\n",
    "df_test[\"Label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "#downloading GLove word embeddings\n",
    "\n",
    "# !wget http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "URL = \"http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\"\n",
    "response = wget.download(URL, \"glove.6B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py): started\n",
      "  Building wheel for wget (setup.py): finished with status 'done'\n",
      "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9686 sha256=065aacf4efa3012c291b032ab64f40b0928bd327f59c14b759080e6ae879a1d8\n",
      "  Stored in directory: c:\\users\\asmeeta\\appdata\\local\\pip\\cache\\wheels\\a1\\b6\\7c\\0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "with ZipFile('glove.6B', 'r') as f:\n",
    "\n",
    "#extract in current directory\n",
    "    f.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding the words from the word embedding dict as key and value as the vectors\n",
    "import numpy as np\n",
    "\n",
    "def add_words_dict(dict_,filename):\n",
    "    with open(filename, \"r\", encoding=\"utf8\") as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.split(\" \")\n",
    "            \n",
    "            try:\n",
    "                dict_[line[0]] = np.array(line[1:], dtype =float)\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = {}\n",
    "\n",
    "add_words_dict(words,\"glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asmeeta\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "C:\\Users\\Asmeeta\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\Asmeeta\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\Asmeeta\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "C:\\Users\\Asmeeta\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\Asmeeta\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\Asmeeta\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "C:\\Users\\Asmeeta\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\Asmeeta\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\Asmeeta\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stopwords and text cleaning\n",
    "def preprocessing(df):\n",
    "    df[\"Reviews\"] = df[\"Reviews\"].apply(lambda x : \" \".join([word for word in x.split(\" \") if word not in stop_words]))\n",
    "    df[\"Reviews\"] = df[\"Reviews\"].apply(lambda x : re.sub('<.*?>','',x)) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a function that converts a sentence to a list of tokens\n",
    "def convert_to_token_list(x):\n",
    "    tokens = tokenizer.tokenize(x)\n",
    "    lowercase_tokens = [t.lower() for t in tokens]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(t) for t in lowercase_tokens]\n",
    "    actual_tokens = [t for t in lemmatized_tokens if t in words]\n",
    "    \n",
    "    return actual_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that convert a sentence to a list a array of vectors\n",
    "def convert_to_vectors(message, word_dict = words):\n",
    "    converted_to_tokenlist = convert_to_token_list(message)\n",
    "    \n",
    "    converted_to_vectors = []\n",
    "    \n",
    "    for word in converted_to_tokenlist:\n",
    "        if word not in word_dict:\n",
    "            continue\n",
    "            \n",
    "            \n",
    "        vector = word_dict[word]\n",
    "        converted_to_vectors.append(vector)\n",
    "            \n",
    "    return np.array(converted_to_vectors, dtype= float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = convert_to_vectors(\"hello darkness\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 50)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1750"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that converts a dataframe to list of vectors\n",
    "def dataframe_processing(df):\n",
    "    df = preprocessing(df)\n",
    "    label = df[\"Label\"].to_numpy().astype(int)\n",
    "    \n",
    "    word_vectors = []\n",
    "    \n",
    "    for review in df[\"Reviews\"]:\n",
    "        converted_sentence_tovec = convert_to_vectors(review)\n",
    "        \n",
    "        if converted_sentence_tovec.shape[0]==0:\n",
    "            converted_sentence_tovec = np.zeros(shape = (1,50))\n",
    "            \n",
    "        word_vectors.append(converted_sentence_tovec)\n",
    "        \n",
    "    return word_vectors, label\n",
    "            \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df_train, df_test]\n",
    "df = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3500, 750, 750)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_1 = int(len(df) *0.7)\n",
    "split_2 = int(len(df) *0.85)\n",
    "\n",
    "train_df, test_df,val_df = df[0:split_1], df[split_1:split_2], df[split_2:]\n",
    "\n",
    "len(train_df), len(test_df), len(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asmeeta\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\Asmeeta\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = dataframe_processing(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3500, 85)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.801e+03, 1.087e+03, 3.500e+02, 1.530e+02, 5.700e+01, 4.000e+01,\n",
       "        9.000e+00, 1.000e+00, 1.000e+00, 1.000e+00]),\n",
       " array([  5. , 103.2, 201.4, 299.6, 397.8, 496. , 594.2, 692.4, 790.6,\n",
       "        888.8, 987. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#trying to find the maximum lenght of sentences\n",
    "sequence_length = []\n",
    "for i in range(len(X_train)):\n",
    "    sequence_length.append(len(X_train[i]))\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3500.000000\n",
       "mean      134.938571\n",
       "std       100.611630\n",
       "min         5.000000\n",
       "25%        74.000000\n",
       "50%       101.000000\n",
       "75%       162.000000\n",
       "max       987.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(sequence_length).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ = 1000\n",
    "#inserting zeros to the vectors so that all the sentences have the same length\n",
    "from copy import deepcopy\n",
    "\n",
    "def padding(X, desired_sequence_len = 1000):\n",
    "    x_copy = deepcopy(X)\n",
    "    \n",
    "    for i, x in enumerate(X):\n",
    "        seq_len = x.shape[0]\n",
    "        seq_len_diff = desired_sequence_len - seq_len\n",
    "        \n",
    "        pad = np.zeros(shape = (seq_len_diff, 50))\n",
    "        \n",
    "        x_copy[i] = np.concatenate([x,pad])\n",
    "        \n",
    "    return np.array(x_copy).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = padding(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3500, 1000, 50)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3500,)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asmeeta\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\Asmeeta\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((750, 1000, 50), (750,))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test, y_test = dataframe_processing(test_df)\n",
    "X_test = padding(X_test)\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asmeeta\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\Asmeeta\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((750, 1000, 50), (750,))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val, y_val = dataframe_processing(val_df)\n",
    "X_val = padding(X_val)\n",
    "X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "model = Sequential([])\n",
    "\n",
    "model.add(layers.Input(shape=(1000,50)))\n",
    "model.add(layers.LSTM(64, return_sequences = True))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.LSTM(64, return_sequences = True))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1, activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_2 (LSTM)               (None, 1000, 64)          29440     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1000, 64)          0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 1000, 64)          33024     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 1000, 64)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 64000)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 64001     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 126,465\n",
      "Trainable params: 126,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflwo.keras.metrics \n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "cp = ModelCheckpoint('model/', save_best_only = True)\n",
    "\n",
    "model.compile(optimizer = Adam(learning_rate = 0.001),\n",
    "             loss = BinaryCrossentropy(),\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.5336 - accuracy: 0.7320"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 132s 1s/step - loss: 0.5336 - accuracy: 0.7320 - val_loss: 0.5677 - val_accuracy: 0.6933\n",
      "Epoch 2/5\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.4413 - accuracy: 0.7971"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 121s 1s/step - loss: 0.4413 - accuracy: 0.7971 - val_loss: 0.4098 - val_accuracy: 0.8080\n",
      "Epoch 3/5\n",
      "110/110 [==============================] - 132s 1s/step - loss: 0.4030 - accuracy: 0.8191 - val_loss: 0.7206 - val_accuracy: 0.6627\n",
      "Epoch 4/5\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.3653 - accuracy: 0.8389"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 136s 1s/step - loss: 0.3653 - accuracy: 0.8389 - val_loss: 0.3694 - val_accuracy: 0.8293\n",
      "Epoch 5/5\n",
      "110/110 [==============================] - 170s 2s/step - loss: 0.3107 - accuracy: 0.8663 - val_loss: 0.6889 - val_accuracy: 0.6720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x193c28c0550>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data = (X_val, y_val), epochs = 5, callbacks = [cp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "best_model = load_model(\"model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 9s 394ms/step\n"
     ]
    }
   ],
   "source": [
    "test_prediction = (best_model.predict(X_test)>0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.81      0.86       500\n",
      "           1       0.69      0.87      0.77       250\n",
      "\n",
      "    accuracy                           0.83       750\n",
      "   macro avg       0.81      0.84      0.82       750\n",
      "weighted avg       0.85      0.83      0.83       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_prediction\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test,test_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
